# 03 - LLMã¨ã®çµ±åˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

## ğŸ“– æ¦‚è¦

ã“ã®ç¯€ã§ã¯ã€Large Language Model (LLM) ã¨Model Context Protocol (MCP) ã‚’çµ±åˆã—ãŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚OpenAI API ã‚„ Claude API ã¨é€£æºã—ã€è‡ªç„¶è¨€èªã§MCPãƒ„ãƒ¼ãƒ«ã‚’æ“ä½œã§ãã‚‹ä¼šè©±ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

## ğŸ¯ å­¦ç¿’ç›®æ¨™

ã“ã®ç¯€ã‚’å®Œäº†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š

- LLMã¨MCPã‚µãƒ¼ãƒãƒ¼ã‚’çµ±åˆã—ãŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆã§ãã‚‹
- è‡ªç„¶è¨€èªã§ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚’æŒ‡ç¤ºã§ãã‚‹
- LLMãŒãƒ„ãƒ¼ãƒ«çµæœã‚’è§£é‡ˆã—ã¦å¿œç­”ã‚’ç”Ÿæˆã§ãã‚‹
- ä¼šè©±ã®æ–‡è„ˆã‚’ä¿æŒã—ãŸå¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã‚‹
- è¤‡æ•°ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¯¾å¿œã§ãã‚‹

## ğŸ› ï¸ å‰ææ¡ä»¶

- [02-client](../02-client/) ãŒå®Œäº†ã—ã¦ã„ã‚‹ã“ã¨
- OpenAI API ã‚­ãƒ¼ã¾ãŸã¯ Anthropic API ã‚­ãƒ¼ã‚’å–å¾—ã—ã¦ã„ã‚‹ã“ã¨
- Node.js (v18ä»¥é™) ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨

## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è¨­å®š

### 1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ

```bash
mkdir mcp-llm-client
cd mcp-llm-client

# package.jsonã®åˆæœŸåŒ–
npm init -y
```

### 2. å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# MCP SDK ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
npm install @modelcontextprotocol/sdk

# LLM API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
npm install openai @anthropic-ai/sdk

# ãã®ä»–ã®ä¾å­˜é–¢ä¿‚
npm install dotenv

# é–‹ç™ºç”¨ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
npm install -D typescript @types/node tsx

# TypeScriptè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
npx tsc --init
```

### 3. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

```bash
# .env ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
touch .env
```

```env
# .env
# OpenAI APIè¨­å®š
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic APIè¨­å®š  
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ (openai | anthropic)
DEFAULT_LLM_PROVIDER=openai

# MCPã‚µãƒ¼ãƒãƒ¼è¨­å®š
MCP_SERVER_COMMAND=node
MCP_SERVER_ARGS=../first-mcp-server/dist/server.js

# ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
DEBUG_MODE=true
```

## ğŸš€ LLMçµ±åˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å®Ÿè£…

### 1. LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æŠ½è±¡åŒ–

```typescript
// src/llm/base-provider.ts
export interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface LLMResponse {
  content: string;
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

export interface ToolCall {
  name: string;
  arguments: Record<string, any>;
}

export abstract class LLMProvider {
  abstract generateResponse(
    messages: LLMMessage[],
    availableTools?: any[]
  ): Promise<LLMResponse>;

  abstract parseToolCalls(content: string): ToolCall[];

  protected formatToolsForLLM(tools: any[]): string {
    return tools.map(tool => {
      const params = Object.entries(tool.inputSchema.properties || {})
        .map(([name, schema]: [string, any]) => {
          const required = tool.inputSchema.required?.includes(name) ? ' (å¿…é ˆ)' : '';
          return `  - ${name}${required}: ${schema.description || 'No description'}`;
        })
        .join('\n');

      return `**${tool.name}**: ${tool.description}\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:\n${params}`;
    }).join('\n\n');
  }
}
```

### 2. OpenAI ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å®Ÿè£…

```typescript
// src/llm/openai-provider.ts
import OpenAI from 'openai';
import { LLMProvider, LLMMessage, LLMResponse, ToolCall } from './base-provider.js';

export class OpenAIProvider extends LLMProvider {
  private client: OpenAI;
  private model: string;

  constructor(apiKey: string, model: string = 'gpt-4') {
    super();
    this.client = new OpenAI({ apiKey });
    this.model = model;
  }

  async generateResponse(
    messages: LLMMessage[],
    availableTools?: any[]
  ): Promise<LLMResponse> {
    try {
      // ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’è¿½åŠ 
      const systemMessage = this.buildSystemMessage(availableTools);
      const chatMessages = [systemMessage, ...messages];

      const response = await this.client.chat.completions.create({
        model: this.model,
        messages: chatMessages.map(msg => ({
          role: msg.role,
          content: msg.content
        })),
        temperature: 0.7,
        max_tokens: 1000
      });

      const choice = response.choices[0];
      if (!choice.message.content) {
        throw new Error('No content in OpenAI response');
      }

      return {
        content: choice.message.content,
        usage: {
          prompt_tokens: response.usage?.prompt_tokens || 0,
          completion_tokens: response.usage?.completion_tokens || 0,
          total_tokens: response.usage?.total_tokens || 0
        }
      };

    } catch (error) {
      throw new Error(`OpenAI API error: ${error}`);
    }
  }

  parseToolCalls(content: string): ToolCall[] {
    const toolCalls: ToolCall[] = [];
    
    // ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
    const toolCallPattern = /```tool:(\w+)\s*(.*?)```/gs;
    let match;

    while ((match = toolCallPattern.exec(content)) !== null) {
      const toolName = match[1];
      const argsText = match[2].trim();

      try {
        const args = argsText ? JSON.parse(argsText) : {};
        toolCalls.push({
          name: toolName,
          arguments: args
        });
      } catch (error) {
        console.warn(`Failed to parse tool arguments: ${argsText}`);
      }
    }

    return toolCalls;
  }

  private buildSystemMessage(availableTools?: any[]): LLMMessage {
    let systemContent = `ã‚ãªãŸã¯ã€MCPãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã§ãã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«å¿œã˜ã¦ã€ä»¥ä¸‹ã®å½¢å¼ã§ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™ã“ã¨ãŒã§ãã¾ã™ï¼š

\`\`\`tool:ãƒ„ãƒ¼ãƒ«å
{
  "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å": "å€¤"
}
\`\`\`

ä¾‹ï¼š
\`\`\`tool:echo
{
  "message": "Hello World"
}
\`\`\``;

    if (availableTools && availableTools.length > 0) {
      systemContent += '\n\nåˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:\n\n' + this.formatToolsForLLM(availableTools);
    }

    systemContent += '\n\nãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹éš›ã¯ã€å¿…ãšä¸Šè¨˜ã®å½¢å¼ã«å¾“ã£ã¦ãã ã•ã„ã€‚ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå¾Œã€ãã®çµæœã‚’è¸ã¾ãˆã¦é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚';

    return {
      role: 'system',
      content: systemContent
    };
  }
}
```

### 3. Anthropic ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å®Ÿè£…

```typescript
// src/llm/anthropic-provider.ts
import Anthropic from '@anthropic-ai/sdk';
import { LLMProvider, LLMMessage, LLMResponse, ToolCall } from './base-provider.js';

export class AnthropicProvider extends LLMProvider {
  private client: Anthropic;
  private model: string;

  constructor(apiKey: string, model: string = 'claude-3-sonnet-20240229') {
    super();
    this.client = new Anthropic({ apiKey });
    this.model = model;
  }

  async generateResponse(
    messages: LLMMessage[],
    availableTools?: any[]
  ): Promise<LLMResponse> {
    try {
      // ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ†é›¢
      const systemMessage = this.buildSystemMessage(availableTools);
      const userMessages = messages.filter(m => m.role !== 'system');

      const response = await this.client.messages.create({
        model: this.model,
        max_tokens: 1000,
        system: systemMessage.content,
        messages: userMessages.map(msg => ({
          role: msg.role === 'user' ? 'user' as const : 'assistant' as const,
          content: msg.content
        }))
      });

      const content = response.content[0];
      if (content.type !== 'text') {
        throw new Error('Non-text response from Claude');
      }

      return {
        content: content.text,
        usage: {
          prompt_tokens: response.usage.input_tokens,
          completion_tokens: response.usage.output_tokens,
          total_tokens: response.usage.input_tokens + response.usage.output_tokens
        }
      };

    } catch (error) {
      throw new Error(`Anthropic API error: ${error}`);
    }
  }

  parseToolCalls(content: string): ToolCall[] {
    const toolCalls: ToolCall[] = [];
    
    // ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
    const toolCallPattern = /<tool:(\w+)>(.*?)<\/tool:\1>/gs;
    let match;

    while ((match = toolCallPattern.exec(content)) !== null) {
      const toolName = match[1];
      const argsText = match[2].trim();

      try {
        const args = argsText ? JSON.parse(argsText) : {};
        toolCalls.push({
          name: toolName,
          arguments: args
        });
      } catch (error) {
        console.warn(`Failed to parse tool arguments: ${argsText}`);
      }
    }

    return toolCalls;
  }

  private buildSystemMessage(availableTools?: any[]): LLMMessage {
    let systemContent = `ã‚ãªãŸã¯ã€MCPãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã§ãã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«å¿œã˜ã¦ã€ä»¥ä¸‹ã®å½¢å¼ã§ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™ã“ã¨ãŒã§ãã¾ã™ï¼š

<tool:ãƒ„ãƒ¼ãƒ«å>
{
  "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å": "å€¤"
}
</tool:ãƒ„ãƒ¼ãƒ«å>

ä¾‹ï¼š
<tool:echo>
{
  "message": "Hello World"
}
</tool:echo>`;

    if (availableTools && availableTools.length > 0) {
      systemContent += '\n\nåˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:\n\n' + this.formatToolsForLLM(availableTools);
    }

    systemContent += '\n\nãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹éš›ã¯ã€å¿…ãšä¸Šè¨˜ã®å½¢å¼ã«å¾“ã£ã¦ãã ã•ã„ã€‚ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå¾Œã€ãã®çµæœã‚’è¸ã¾ãˆã¦é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚';

    return {
      role: 'system',
      content: systemContent
    };
  }
}
```

### 4. ãƒ¡ã‚¤ãƒ³çµ±åˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å®Ÿè£…

```typescript
// src/llm-client.ts
import { MCPClient } from '../02-client/src/client.js';
import { LLMProvider, LLMMessage } from './llm/base-provider.js';
import { OpenAIProvider } from './llm/openai-provider.js';
import { AnthropicProvider } from './llm/anthropic-provider.js';
import * as dotenv from 'dotenv';

dotenv.config();

export interface LLMClientConfig {
  mcpServerCommand: string;
  mcpServerArgs?: string[];
  llmProvider: 'openai' | 'anthropic';
  llmModel?: string;
  debug?: boolean;
}

export class LLMMCPClient {
  private mcpClient: MCPClient;
  private llmProvider: LLMProvider;
  private conversationHistory: LLMMessage[] = [];
  private availableTools: any[] = [];
  private config: LLMClientConfig;

  constructor(config: LLMClientConfig) {
    this.config = config;

    // MCP ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
    this.mcpClient = new MCPClient({
      serverCommand: config.mcpServerCommand,
      serverArgs: config.mcpServerArgs,
      debug: config.debug
    });

    // LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–
    this.llmProvider = this.createLLMProvider(config);
  }

  private createLLMProvider(config: LLMClientConfig): LLMProvider {
    switch (config.llmProvider) {
      case 'openai':
        if (!process.env.OPENAI_API_KEY) {
          throw new Error('OPENAI_API_KEY is required');
        }
        return new OpenAIProvider(process.env.OPENAI_API_KEY, config.llmModel);
      
      case 'anthropic':
        if (!process.env.ANTHROPIC_API_KEY) {
          throw new Error('ANTHROPIC_API_KEY is required');
        }
        return new AnthropicProvider(process.env.ANTHROPIC_API_KEY, config.llmModel);
      
      default:
        throw new Error(`Unsupported LLM provider: ${config.llmProvider}`);
    }
  }

  /**
   * ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
   */
  async initialize(): Promise<void> {
    try {
      console.log('ğŸš€ LLM-MCP Client ã‚’åˆæœŸåŒ–ä¸­...');

      // MCPã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶š
      await this.mcpClient.connect();
      console.log('âœ… MCPã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã—ã¾ã—ãŸ');

      // åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’å–å¾—
      this.availableTools = await this.mcpClient.listTools();
      console.log(`ğŸ“‹ ${this.availableTools.length} å€‹ã®ãƒ„ãƒ¼ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã§ã™`);

      if (this.config.debug) {
        console.log('åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:', this.availableTools.map(t => t.name));
      }

    } catch (error) {
      console.error('âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼:', error);
      throw error;
    }
  }

  /**
   * ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å‡¦ç†
   */
  async processMessage(userMessage: string): Promise<string> {
    try {
      // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
      this.conversationHistory.push({
        role: 'user',
        content: userMessage
      });

      console.log(`ğŸ’¬ ãƒ¦ãƒ¼ã‚¶ãƒ¼: ${userMessage}`);

      // LLMã‹ã‚‰å¿œç­”ã‚’ç”Ÿæˆ
      let response = await this.llmProvider.generateResponse(
        this.conversationHistory,
        this.availableTools
      );

      // ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
      const toolCalls = this.llmProvider.parseToolCalls(response.content);

      if (toolCalls.length > 0) {
        console.log(`ğŸ”§ ${toolCalls.length} å€‹ã®ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œä¸­...`);

        // ãƒ„ãƒ¼ãƒ«ã‚’é †æ¬¡å®Ÿè¡Œ
        for (const toolCall of toolCalls) {
          console.log(`  â†’ ${toolCall.name}(${JSON.stringify(toolCall.arguments)})`);
          
          const toolResult = await this.mcpClient.callTool(
            toolCall.name,
            toolCall.arguments
          );

          if (toolResult.success) {
            const resultText = toolResult.content?.map(c => c.text).join('\n') || '';
            console.log(`  âœ… çµæœ: ${resultText.substring(0, 100)}...`);

            // ãƒ„ãƒ¼ãƒ«çµæœã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            this.conversationHistory.push({
              role: 'assistant',
              content: `ãƒ„ãƒ¼ãƒ« "${toolCall.name}" ã‚’å®Ÿè¡Œã—ã¾ã—ãŸã€‚çµæœ: ${resultText}`
            });
          } else {
            console.log(`  âŒ ã‚¨ãƒ©ãƒ¼: ${toolResult.error}`);
            
            this.conversationHistory.push({
              role: 'assistant',
              content: `ãƒ„ãƒ¼ãƒ« "${toolCall.name}" ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: ${toolResult.error}`
            });
          }
        }

        // ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå¾Œã«å†åº¦LLMã‹ã‚‰å¿œç­”ã‚’ç”Ÿæˆ
        response = await this.llmProvider.generateResponse(
          this.conversationHistory,
          this.availableTools
        );
      }

      // æœ€çµ‚çš„ãªå¿œç­”ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
      this.conversationHistory.push({
        role: 'assistant',
        content: response.content
      });

      if (this.config.debug && response.usage) {
        console.log(`ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: ${response.usage.total_tokens}`);
      }

      return response.content;

    } catch (error) {
      console.error('âŒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ã‚¨ãƒ©ãƒ¼:', error);
      return `ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: ${error}`;
    }
  }

  /**
   * ä¼šè©±å±¥æ­´ã®å–å¾—
   */
  getConversationHistory(): LLMMessage[] {
    return [...this.conversationHistory];
  }

  /**
   * ä¼šè©±å±¥æ­´ã®ã‚¯ãƒªã‚¢
   */
  clearConversationHistory(): void {
    this.conversationHistory = [];
    console.log('ğŸ—‘ï¸  ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ');
  }

  /**
   * ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®çµ‚äº†å‡¦ç†
   */
  async shutdown(): Promise<void> {
    try {
      await this.mcpClient.disconnect();
      console.log('ğŸ‘‹ LLM-MCP Client ã‚’çµ‚äº†ã—ã¾ã—ãŸ');
    } catch (error) {
      console.error('âŒ çµ‚äº†å‡¦ç†ã‚¨ãƒ©ãƒ¼:', error);
    }
  }
}
```

## ğŸ§ª å®Ÿéš›ã®ä½¿ç”¨ä¾‹

### 1. ã‚·ãƒ³ãƒ—ãƒ«ãªä¼šè©±ä¾‹

```typescript
// src/examples/simple-conversation.ts
import { LLMMCPClient } from '../llm-client.js';

async function simpleConversation() {
  const client = new LLMMCPClient({
    mcpServerCommand: process.env.MCP_SERVER_COMMAND || 'node',
    mcpServerArgs: [process.env.MCP_SERVER_ARGS || '../first-mcp-server/dist/server.js'],
    llmProvider: (process.env.DEFAULT_LLM_PROVIDER as 'openai' | 'anthropic') || 'openai',
    debug: true
  });

  try {
    await client.initialize();

    // åŸºæœ¬çš„ãªæŒ¨æ‹¶
    let response = await client.processMessage('ã“ã‚“ã«ã¡ã¯ï¼ä½•ãŒã§ãã¾ã™ã‹ï¼Ÿ');
    console.log('ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:', response);

    // è¨ˆç®—ã®ä¾é ¼
    response = await client.processMessage('15ã¨27ã‚’è¶³ã—ã¦ãã ã•ã„');
    console.log('ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:', response);

    // ç¾åœ¨æ™‚åˆ»ã®ç¢ºèª
    response = await client.processMessage('ä»Šä½•æ™‚ã§ã™ã‹ï¼Ÿ');
    console.log('ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:', response);

    // ã‚¨ã‚³ãƒ¼æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ
    response = await client.processMessage('ã€ŒHello, MCP World!ã€ã¨ã‚¨ã‚³ãƒ¼ã—ã¦ãã ã•ã„');
    console.log('ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:', response);

  } catch (error) {
    console.error('ã‚¨ãƒ©ãƒ¼:', error);
  } finally {
    await client.shutdown();
  }
}

simpleConversation().catch(console.error);
```

### 2. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªä¼šè©±ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

```typescript
// src/examples/interactive-chat.ts
import { LLMMCPClient } from '../llm-client.js';
import * as readline from 'readline';

class InteractiveChat {
  private client: LLMMCPClient;
  private rl: readline.Interface;

  constructor() {
    this.client = new LLMMCPClient({
      mcpServerCommand: process.env.MCP_SERVER_COMMAND || 'node',
      mcpServerArgs: [process.env.MCP_SERVER_ARGS || '../first-mcp-server/dist/server.js'],
      llmProvider: (process.env.DEFAULT_LLM_PROVIDER as 'openai' | 'anthropic') || 'openai',
      debug: false
    });

    this.rl = readline.createInterface({
      input: process.stdin,
      output: process.stdout
    });
  }

  async start(): Promise<void> {
    try {
      console.log('ğŸš€ LLM-MCP Interactive Chat ã‚’é–‹å§‹ã—ã¾ã™...\n');
      
      await this.client.initialize();
      
      console.log('âœ… æº–å‚™å®Œäº†ï¼ä½•ã§ã‚‚èã„ã¦ãã ã•ã„ã€‚\n');
      console.log('ğŸ’¡ ãƒ’ãƒ³ãƒˆ:');
      console.log('  - "è¨ˆç®—ã—ã¦" ã‚„ "æ™‚åˆ»ã‚’æ•™ãˆã¦" ãªã©è‡ªç„¶ãªè¨€è‘‰ã§è©±ã—ã‹ã‘ã¦ãã ã•ã„');
      console.log('  - "/help" ã§ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º');
      console.log('  - "/clear" ã§ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢');
      console.log('  - "/quit" ã§çµ‚äº†\n');

      this.handleUserInput();

    } catch (error) {
      console.error('âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼:', error);
      process.exit(1);
    }
  }

  private handleUserInput(): void {
    this.rl.question('ğŸ’¬ ã‚ãªãŸ: ', async (input) => {
      const trimmedInput = input.trim();

      // ã‚³ãƒãƒ³ãƒ‰ã®å‡¦ç†
      if (trimmedInput.startsWith('/')) {
        await this.handleCommand(trimmedInput);
        this.handleUserInput();
        return;
      }

      // ç©ºå…¥åŠ›ã®ãƒã‚§ãƒƒã‚¯
      if (!trimmedInput) {
        console.log('â“ ä½•ã‹å…¥åŠ›ã—ã¦ãã ã•ã„\n');
        this.handleUserInput();
        return;
      }

      try {
        // LLMã«é€ä¿¡ã—ã¦å¿œç­”ã‚’å–å¾—
        console.log('ğŸ¤” è€ƒãˆä¸­...\n');
        const response = await this.client.processMessage(trimmedInput);
        console.log('ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:', response);
        console.log();

      } catch (error) {
        console.error('âŒ ã‚¨ãƒ©ãƒ¼:', error);
        console.log();
      }

      this.handleUserInput();
    });
  }

  private async handleCommand(command: string): Promise<void> {
    switch (command) {
      case '/help':
        this.showHelp();
        break;
      
      case '/clear':
        this.client.clearConversationHistory();
        console.log('âœ… ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ\n');
        break;
      
      case '/history':
        this.showHistory();
        break;
      
      case '/quit':
      case '/exit':
        await this.quit();
        return;
      
      default:
        console.log('â“ ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰ã§ã™ã€‚/help ã§ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤ºã—ã¦ãã ã•ã„\n');
    }
  }

  private showHelp(): void {
    console.log('ğŸ“– åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰:');
    console.log('  /help    - ã“ã®ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º');
    console.log('  /clear   - ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢');
    console.log('  /history - ä¼šè©±å±¥æ­´ã‚’è¡¨ç¤º');
    console.log('  /quit    - ãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†');
    console.log();
    console.log('ğŸ’¡ ä½¿ç”¨ä¾‹:');
    console.log('  - "10ã¨20ã‚’è¶³ã—ã¦ãã ã•ã„"');
    console.log('  - "ç¾åœ¨ã®æ™‚åˆ»ã‚’æ•™ãˆã¦ãã ã•ã„"');
    console.log('  - "Hello Worldã¨ã‚¨ã‚³ãƒ¼ã—ã¦ãã ã•ã„"');
    console.log();
  }

  private showHistory(): void {
    const history = this.client.getConversationHistory();
    
    if (history.length === 0) {
      console.log('ğŸ“ ä¼šè©±å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“\n');
      return;
    }

    console.log('ğŸ“ ä¼šè©±å±¥æ­´:');
    history.forEach((message, index) => {
      const speaker = message.role === 'user' ? 'ğŸ’¬ ã‚ãªãŸ' : 'ğŸ¤– ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ';
      const content = message.content.substring(0, 100);
      console.log(`  ${index + 1}. ${speaker}: ${content}${message.content.length > 100 ? '...' : ''}`);
    });
    console.log();
  }

  private async quit(): Promise<void> {
    console.log('\nğŸ‘‹ ãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™...');
    await this.client.shutdown();
    this.rl.close();
    process.exit(0);
  }
}

// ãƒãƒ£ãƒƒãƒˆã®é–‹å§‹
const chat = new InteractiveChat();
chat.start().catch(console.error);

// Ctrl+C ã§ã®ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«çµ‚äº†
process.on('SIGINT', async () => {
  console.log('\n\nğŸ‘‹ ãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™...');
  process.exit(0);
});
```

### 3. è¤‡æ•°LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

```typescript
// src/examples/llm-benchmark.ts
import { LLMMCPClient } from '../llm-client.js';

async function benchmarkLLMProviders() {
  const testQueries = [
    '15ã¨27ã‚’è¶³ã—ã¦ãã ã•ã„',
    'ç¾åœ¨ã®æ™‚åˆ»ã‚’æ•™ãˆã¦ãã ã•ã„',
    'Hello MCP Worldã¨ã‚¨ã‚³ãƒ¼ã—ã¦ãã ã•ã„',
    '100ã‹ã‚‰25ã‚’å¼•ã„ãŸçµæœã‚’æ•™ãˆã¦ãã ã•ã„'
  ];

  const providers: Array<'openai' | 'anthropic'> = ['openai', 'anthropic'];

  for (const provider of providers) {
    console.log(`\nğŸ§ª ${provider.toUpperCase()} ã®ãƒ†ã‚¹ãƒˆé–‹å§‹...\n`);

    const client = new LLMMCPClient({
      mcpServerCommand: process.env.MCP_SERVER_COMMAND || 'node',
      mcpServerArgs: [process.env.MCP_SERVER_ARGS || '../first-mcp-server/dist/server.js'],
      llmProvider: provider,
      debug: false
    });

    try {
      await client.initialize();

      for (let i = 0; i < testQueries.length; i++) {
        const query = testQueries[i];
        console.log(`ğŸ“ ã‚¯ã‚¨ãƒª ${i + 1}: ${query}`);
        
        const startTime = Date.now();
        const response = await client.processMessage(query);
        const duration = Date.now() - startTime;
        
        console.log(`â±ï¸  å®Ÿè¡Œæ™‚é–“: ${duration}ms`);
        console.log(`ğŸ¤– å¿œç­”: ${response.substring(0, 200)}...\n`);
      }

    } catch (error) {
      console.error(`âŒ ${provider} ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼:`, error);
    } finally {
      await client.shutdown();
    }
  }
}

benchmarkLLMProviders().catch(console.error);
```

### 4. package.json ã®æ›´æ–°

```json
{
  "name": "mcp-llm-client",
  "version": "1.0.0",
  "description": "MCP LLM Integration Client",
  "main": "dist/llm-client.js",
  "type": "module",
  "scripts": {
    "build": "tsc",
    "dev": "tsx src/examples/simple-conversation.ts",
    "chat": "tsx src/examples/interactive-chat.ts",
    "benchmark": "tsx src/examples/llm-benchmark.ts"
  },
  "keywords": ["mcp", "llm", "openai", "anthropic", "ai"],
  "author": "Your Name",
  "license": "MIT",
  "dependencies": {
    "@modelcontextprotocol/sdk": "^0.4.0",
    "openai": "^4.0.0",
    "@anthropic-ai/sdk": "^0.20.0",
    "dotenv": "^16.0.0"
  },
  "devDependencies": {
    "@types/node": "^20.0.0",
    "tsx": "^4.0.0",
    "typescript": "^5.0.0"
  }
}
```

## ğŸ§ª ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ

### 1. ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª

```bash
# .envãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
cat .env

# å¿…è¦ãªAPIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
echo $OPENAI_API_KEY
echo $ANTHROPIC_API_KEY
```

### 2. åŸºæœ¬ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ

```bash
# ãƒ“ãƒ«ãƒ‰
npm run build

# ã‚·ãƒ³ãƒ—ãƒ«ãªä¼šè©±ãƒ†ã‚¹ãƒˆ
npm run dev

# ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒƒãƒˆ
npm run chat

# LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
npm run benchmark
```

## ğŸ” ã‚³ãƒ¼ãƒ‰ã®è§£èª¬

### 1. LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æŠ½è±¡åŒ–

```typescript
export abstract class LLMProvider {
  abstract generateResponse(messages: LLMMessage[], availableTools?: any[]): Promise<LLMResponse>;
  abstract parseToolCalls(content: string): ToolCall[];
}
```

ã“ã®æŠ½è±¡åŒ–ã«ã‚ˆã‚Šã€ç•°ãªã‚‹LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’çµ±ä¸€çš„ã«æ‰±ãˆã¾ã™ã€‚

### 2. ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®è§£æ

```typescript
// OpenAIç”¨ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
const toolCallPattern = /```tool:(\w+)\s*(.*?)```/gs;

// Anthropicç”¨ã®ãƒ‘ã‚¿ãƒ¼ãƒ³  
const toolCallPattern = /<tool:(\w+)>(.*?)<\/tool:\1>/gs;
```

å„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«é©ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’è§£æã—ã¾ã™ã€‚

### 3. ä¼šè©±ãƒ•ãƒ­ãƒ¼ã®ç®¡ç†

```typescript
// ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ â†’ LLM â†’ ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ â†’ LLM â†’ æœ€çµ‚å›ç­”
this.conversationHistory.push({ role: 'user', content: userMessage });
let response = await this.llmProvider.generateResponse(this.conversationHistory, this.availableTools);
const toolCalls = this.llmProvider.parseToolCalls(response.content);
// ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ...
response = await this.llmProvider.generateResponse(this.conversationHistory, this.availableTools);
```

## ğŸš¨ ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

### 1. APIã‚­ãƒ¼ã‚¨ãƒ©ãƒ¼

```bash
# ã‚¨ãƒ©ãƒ¼ä¾‹
Error: OPENAI_API_KEY is required

# è§£æ±ºæ–¹æ³•
# .envãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèªã¨è¨­å®š
echo "OPENAI_API_KEY=your_api_key_here" >> .env
```

### 2. ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®è§£æå¤±æ•—

```typescript
// ãƒ‡ãƒãƒƒã‚°ç”¨ã®ãƒ­ã‚°è¿½åŠ 
if (this.config.debug) {
  console.log('LLM Response:', response.content);
  console.log('Parsed Tool Calls:', toolCalls);
}
```

### 3. ä¼šè©±å±¥æ­´ã®è‚¥å¤§åŒ–

```typescript
// ä¼šè©±å±¥æ­´ã®åˆ¶é™
private limitConversationHistory(maxMessages: number = 20): void {
  if (this.conversationHistory.length > maxMessages) {
    this.conversationHistory = this.conversationHistory.slice(-maxMessages);
  }
}
```

## ğŸ¯ æ¼”ç¿’èª²é¡Œ

### åˆç´šèª²é¡Œ

1. **æ–°ã—ã„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼**: Google Gemini APIã«å¯¾å¿œã—ãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã¦ãã ã•ã„
2. **å¿œç­”æ™‚é–“è¨ˆæ¸¬**: å„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å¿œç­”æ™‚é–“ã‚’è¨ˆæ¸¬ãƒ»è¡¨ç¤ºã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ãã ã•ã„
3. **è¨­å®šå¯èƒ½ãªã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤–éƒ¨è¨­å®šã§å¤‰æ›´ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„

### ä¸­ç´šèª²é¡Œ

1. **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†**: ä¼šè©±ã®é•·æœŸè¨˜æ†¶æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„
2. **ãƒãƒ«ãƒãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ**: è¤‡æ•°ã®ãƒ„ãƒ¼ãƒ«ã‚’ä¸¦è¡Œã—ã¦å®Ÿè¡Œã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ãã ã•ã„
3. **ã‚¨ãƒ©ãƒ¼å›å¾©**: ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå¤±æ•—æ™‚ã®è‡ªå‹•å›å¾©æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„

## ğŸ‰ ã¾ã¨ã‚

ã“ã®ç¯€ã§ã¯ã€LLMã¨MCPã®çµ±åˆã«ã¤ã„ã¦å­¦ç¿’ã—ã¾ã—ãŸï¼š

### å­¦ç¿’ã—ãŸå†…å®¹

1. **LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æŠ½è±¡åŒ–**: è¤‡æ•°ã®LLMã‚’çµ±ä¸€çš„ã«æ‰±ã†è¨­è¨ˆ
2. **è‡ªç„¶è¨€èªã§ã®ãƒ„ãƒ¼ãƒ«æ“ä½œ**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç„¶ãªæŒ‡ç¤ºã‚’ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã«å¤‰æ›
3. **ä¼šè©±ãƒ•ãƒ­ãƒ¼ã®ç®¡ç†**: æ–‡è„ˆã‚’ä¿æŒã—ãŸå¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰
4. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: å …ç‰¢ãªã‚¨ãƒ©ãƒ¼å‡¦ç†ã®å®Ÿè£…

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

LLMã¨MCPã®åŸºæœ¬çš„ãªçµ±åˆãŒã§ããŸã‚‰ã€**[04-vscode](../04-vscode/)** ã«é€²ã‚“ã§é–‹ç™ºç’°å¢ƒã§ã®æ´»ç”¨ã‚’å­¦ç¿’ã—ã¾ã—ã‚‡ã†ã€‚

## ğŸ“š å‚è€ƒè³‡æ–™

- [OpenAI API Documentation](https://platform.openai.com/docs)
- [Anthropic API Documentation](https://docs.anthropic.com/)
- [LangChain Documentation](https://js.langchain.com/docs/)

---

*LLMã¨MCPã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚Šã€è‡ªç„¶è¨€èªã§ãƒ„ãƒ¼ãƒ«ã‚’æ“ä½œã§ãã‚‹å¼·åŠ›ãªã‚·ã‚¹ãƒ†ãƒ ãŒæ§‹ç¯‰ã§ãã¾ã™ã€‚æ§˜ã€…ãªæŒ‡ç¤ºã‚’è©¦ã—ã¦ã€ã‚·ã‚¹ãƒ†ãƒ ã®å¯èƒ½æ€§ã‚’æ¢ã£ã¦ãã ã•ã„ã€‚*